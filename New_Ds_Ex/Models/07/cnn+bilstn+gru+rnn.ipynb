{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1839c0f9-dc52-4d9f-9bad-9aa5e84ba35e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T16:26:30.857233Z",
     "iopub.status.busy": "2023-04-11T16:26:30.856770Z",
     "iopub.status.idle": "2023-04-11T16:26:34.130662Z",
     "shell.execute_reply": "2023-04-11T16:26:34.129427Z",
     "shell.execute_reply.started": "2023-04-11T16:26:30.857190Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-11 21:56:31.843897: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-11 21:56:31.888302: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-11 21:56:31.889305: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-11 21:56:32.988125: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        PM\n",
      "DATE                      \n",
      "2010-01-02 00:00:00  129.0\n",
      "2010-01-02 01:00:00  148.0\n",
      "2010-01-02 02:00:00  159.0\n",
      "2010-01-02 03:00:00  181.0\n",
      "2010-01-02 04:00:00  138.0\n",
      "...                    ...\n",
      "2014-12-31 19:00:00    8.0\n",
      "2014-12-31 20:00:00   10.0\n",
      "2014-12-31 21:00:00   10.0\n",
      "2014-12-31 22:00:00    8.0\n",
      "2014-12-31 23:00:00   12.0\n",
      "\n",
      "[41757 rows x 1 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"reshape\" (type Reshape).\n\ntotal size of new array must be unchanged, input_shape = [704], output_shape = [24, -1]\n\nCall arguments received by layer \"reshape\" (type Reshape):\n  • inputs=tf.Tensor(shape=(None, 704), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m pooling_layer \u001b[38;5;241m=\u001b[39m MaxPooling1D(pool_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)(conv_layer)\n\u001b[1;32m     53\u001b[0m flatten_layer \u001b[38;5;241m=\u001b[39m Flatten()(pooling_layer)\n\u001b[0;32m---> 54\u001b[0m reshape_layer \u001b[38;5;241m=\u001b[39m \u001b[43mReshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflatten_layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m biLSTM_layer \u001b[38;5;241m=\u001b[39m Bidirectional(LSTM(units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))(reshape_layer)\n\u001b[1;32m     56\u001b[0m GRU_layer \u001b[38;5;241m=\u001b[39m GRU(units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(biLSTM_layer)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/layers/reshaping/reshape.py:115\u001b[0m, in \u001b[0;36mReshape._fix_unknown_dimension\u001b[0;34m(self, input_shape, output_shape)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unknown \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m known \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m original \u001b[38;5;241m%\u001b[39m known \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    116\u001b[0m     output_shape[unknown] \u001b[38;5;241m=\u001b[39m original \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m known\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m original \u001b[38;5;241m!=\u001b[39m known:\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"reshape\" (type Reshape).\n\ntotal size of new array must be unchanged, input_shape = [704], output_shape = [24, -1]\n\nCall arguments received by layer \"reshape\" (type Reshape):\n  • inputs=tf.Tensor(shape=(None, 704), dtype=float32)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Reshape, concatenate, Conv1D, MaxPooling1D, Bidirectional, GRU, SimpleRNN, LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# load data and scale\n",
    "#data = pd.read_csv('hourly_data.csv', parse_dates=['datetime'], index_col='datetime')\n",
    "df=pd.read_csv('PRSA_data_2010.1.1-2014.12.31.csv')\n",
    "df=df.iloc[:,1:-7]\n",
    "df[\"Date\"] = pd.to_datetime(df[\"year\"].apply(str) + \"/\" + df[\"month\"].apply(str) + \"/\" + df[\"day\"].apply(str)+ \" \" + df[\"hour\"].apply(str)+ \":\" +\"00\")\n",
    "df=df.drop(['year','month','day','hour'],axis=1)\n",
    "df=df.dropna()\n",
    "df=df.rename(columns={'Date':'DATE','pm2.5':'PM'})\n",
    "data=df.set_index('DATE')\n",
    "print(data)\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# define input shape and number of time steps\n",
    "n_steps = 24\n",
    "n_features = 1\n",
    "n_obs = n_steps*n_features\n",
    "\n",
    "# split into train and test sets\n",
    "train_size = int(len(data) * 0.8)\n",
    "train, test = scaled_data[:train_size,:], scaled_data[train_size:,:]\n",
    "\n",
    "# create time series data for training\n",
    "X_train, y_train = [], []\n",
    "for i in range(n_obs, len(train)):\n",
    "    X_train.append(train[i-n_obs:i, 0])\n",
    "    y_train.append(train[i, 0])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_train = X_train.reshape((X_train.shape[0], n_steps, n_features))\n",
    "\n",
    "# create time series data for testing\n",
    "X_test, y_test = [], []\n",
    "for i in range(n_obs, len(test)):\n",
    "    X_test.append(test[i-n_obs:i, 0])\n",
    "    y_test.append(test[i, 0])\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "X_test = X_test.reshape((X_test.shape[0], n_steps, n_features))\n",
    "\n",
    "# define model architecture\n",
    "input_layer = Input(shape=(n_steps, n_features))\n",
    "conv_layer = Conv1D(filters=64, kernel_size=3, activation='relu')(input_layer)\n",
    "pooling_layer = MaxPooling1D(pool_size=2)(conv_layer)\n",
    "flatten_layer = Flatten()(pooling_layer)\n",
    "reshape_layer = Reshape((n_steps, -1))(flatten_layer)\n",
    "biLSTM_layer = Bidirectional(LSTM(units=50, return_sequences=True))(reshape_layer)\n",
    "GRU_layer = GRU(units=50, return_sequences=True)(biLSTM_layer)\n",
    "RNN_layer = SimpleRNN(units=50, return_sequences=True)(GRU_layer)\n",
    "merge_layer = concatenate([biLSTM_layer, GRU_layer, RNN_layer], axis=-1)\n",
    "flatten2_layer = Flatten()(merge_layer)\n",
    "dropout_layer = Dropout(0.2)(flatten2_layer)\n",
    "output_layer = Dense(units=1, activation='linear')(dropout_layer)\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# define early stopping and compile model\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "adam = Adam(lr=0.0001)\n",
    "model.compile(optimizer=adam, loss='mean_squared_error')\n",
    "\n",
    "# train model and evaluate on test set\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stop])\n",
    "test_loss = model.evaluate(X_test, y_test)\n",
    "\n",
    "# predict on test set and inverse transform\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b7c3d6-f070-490b-b1e3-c504a1016e83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
