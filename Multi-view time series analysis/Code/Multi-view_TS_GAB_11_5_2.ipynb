{"cells":[{"cell_type":"code","execution_count":13,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":592,"status":"ok","timestamp":1686379935502,"user":{"displayName":"Dr. Vipin Kumar Computer Science \u0026 IT MGCUB","userId":"07997955097507838230"},"user_tz":-330},"id":"4u45NlSRPtDc"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\n!pip install --pre pycaret\\n!pip install pycaret\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\\n\""]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","!pip install --pre pycaret\n","!pip install pycaret\n","from google.colab import drive\n","drive.mount('/content/drive')\n","'''"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1686379936361,"user":{"displayName":"Dr. Vipin Kumar Computer Science \u0026 IT MGCUB","userId":"07997955097507838230"},"user_tz":-330},"id":"5W_V6ryPPz05"},"outputs":[],"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","from google.colab import files\n","import pandas as pd\n","import io\n","import numpy as np\n","import os\n","\n","from datetime import datetime\n","#from download import download\n","\n","import seaborn as sns\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import os\n","from google.colab import files\n","import time\n","import numpy as np\n","import pandas as pd\n","#import numpy as np\n","from pycaret.datasets import get_data\n","from pycaret.time_series import TSForecastingExperiment\n","from statsmodels.tsa.stattools import adfuller\n","import plotly.express as px\n","from statsmodels.tsa.stattools import kpss\n","mpl.rcParams['figure.figsize']=(8,6)\n","mpl.rcParams['axes.grid']=False"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4741,"status":"ok","timestamp":1686379941099,"user":{"displayName":"Dr. Vipin Kumar Computer Science \u0026 IT MGCUB","userId":"07997955097507838230"},"user_tz":-330},"id":"OFhoFtPLHUy7","outputId":"225c0bb4-dee6-46d7-a111-5f9a9185807a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1686379941100,"user":{"displayName":"Dr. Vipin Kumar Computer Science \u0026 IT MGCUB","userId":"07997955097507838230"},"user_tz":-330},"id":"g9QcyWNqkMDo"},"outputs":[],"source":["#NECESSARY IMPORTS\n","from math import sqrt\n","from numpy import concatenate\n","from matplotlib import pyplot\n","from pandas import read_csv\n","from pandas import DataFrame\n","from pandas import concat\n","from statsmodels.tsa.api import VAR, adfuller, kpss\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import mean_squared_error\n","from sktime.performance_metrics.forecasting import MeanAbsoluteScaledError\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","import pandas as pd\n","import numpy as np\n","from keras.optimizers import RMSprop\n","from tensorflow.keras import Sequential, Model\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.metrics import RootMeanSquaredError , MeanAbsoluteError\n","from keras.layers import UpSampling1D,Input, Dense, SimpleRNN , Dropout,DepthwiseConv1D,Reshape, Attention, SpatialDropout1D,LSTM,LocallyConnected1D, GlobalMaxPooling1D,AveragePooling1D, Bidirectional, TimeDistributed, GRU, LayerNormalization, ConvLSTM1D\n","from keras.layers.convolutional import MaxPooling1D, Conv1D\n","from keras.layers import Flatten, ConvLSTM2D\n","from keras.regularizers import l1, l2\n","from keras.layers import RepeatVector\n","import time\n","import random\n","#!pip install pandasql\n","import tensorflow as tf\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import mean_absolute_percentage_error\n","from sklearn.metrics import r2_score\n","random.seed(1337)\n","np.random.seed(1337)\n","tf.random.set_seed(1337)\n","#import pandasql as ps\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","#from pandasql import sqldf\n","pysqldf = lambda q: sqldf(q, globals()) \n","from sklearn.metrics import mean_squared_error"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1686379941100,"user":{"displayName":"Dr. Vipin Kumar Computer Science \u0026 IT MGCUB","userId":"07997955097507838230"},"user_tz":-330},"id":"sCq8T6_qKr4Z","outputId":"81abc67b-2c11-4ee0-e125-a4065adc92a4"},"outputs":[{"data":{"text/html":["\n","  \u003cdiv id=\"df-55f284a6-d26d-45fe-a7cf-1eb8b4811e13\"\u003e\n","    \u003cdiv class=\"colab-df-container\"\u003e\n","      \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eMCH_mV\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003e-0.229\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003e-0.234\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003e-0.234\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003e-0.229\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003e-0.227\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e\n","      \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-55f284a6-d26d-45fe-a7cf-1eb8b4811e13')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\"\u003e\n","        \n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\"\u003e\n","    \u003cpath d=\"M0 0h24v24H0V0z\" fill=\"none\"/\u003e\n","    \u003cpath d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/\u003e\u003cpath d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/\u003e\n","  \u003c/svg\u003e\n","      \u003c/button\u003e\n","      \n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","      \u003cscript\u003e\n","        const buttonEl =\n","          document.querySelector('#df-55f284a6-d26d-45fe-a7cf-1eb8b4811e13 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-55f284a6-d26d-45fe-a7cf-1eb8b4811e13');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      \u003c/script\u003e\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n","  "],"text/plain":["   MCH_mV\n","0  -0.229\n","1  -0.234\n","2  -0.234\n","3  -0.229\n","4  -0.227"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","dir_path='/content/drive/MyDrive/AI LAB CoLab/TS_datasets_processed/'\n","\n","Dataset_list=[\"PM2.5_p09_data.csv\",\n","              \"Myocardial_infarction_heart_data.csv\",\n","              \"Tetuan_City_Power_Consumption_Zone1_processed.csv\",\n","              \"Daily_Sunspot_data.csv\",\n","              \"Healthy_control_heart_data.csv\",\n","              \"POWER_Point_Daily_Presure_Data.csv\",\n","              \"POWER_Point_Daily_Temperature_Data.csv\",\n","              \"Metro_Interstate_Traffic_Volume_PD.csv\"]\n","#0,3,4,7,6\n","views_upto=10\n","Start_view=3\n","\n","Dataset_number=1\n","\n","PredictorName='CNN'\n","\n","#name of predictors=['BiLSTM','LSTM','GRU','RNN', 'CNN']\n","path = os.path.join(dir_path, Dataset_list[Dataset_number])\n","\n","df= pd.read_csv(path)\n","df.head()\n"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1686379941100,"user":{"displayName":"Dr. Vipin Kumar Computer Science \u0026 IT MGCUB","userId":"07997955097507838230"},"user_tz":-330},"id":"MMylUp9lB9P0","outputId":"33a81334-47c0-4004-803a-fc9dcffbbd2b"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/MyDrive/AI LAB CoLab/TS_datasets_processed/Myocardial_infarction_heart_data.csv'"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["path"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1686379941101,"user":{"displayName":"Dr. Vipin Kumar Computer Science \u0026 IT MGCUB","userId":"07997955097507838230"},"user_tz":-330},"id":"Wq1XLS0Rd7lf"},"outputs":[],"source":["##########################################################\n","################ Data Preprocessing ######################\n","##########################################################\n","\n","# Below function deals with converting the data into time series data with specific number of lags/window/timesteps\n","\n","def one_step_forecast(df, window):\n","    d = df.values\n","    x = []\n","    n = len(df)\n","    idx = df.index[:-window]\n","    for start in range(n-window):\n","        end = start + window\n","        x.append(d[start:end])\n","    cols = [f'x_{i}' for i in range(1, window+1)]\n","    x = np.array(x).reshape(n-window, -1)\n","    y = df.iloc[window:].values\n","    df_xs = pd.DataFrame(x, columns=cols, index=idx)\n","    df_y = pd.DataFrame(y.reshape(-1), columns=['y'], index=idx)\n","    return pd.concat([df_xs, df_y], axis=1).dropna()\n","\n","\n","# Below class deals with the Z-score Normalization / Splitting of the DataSet\n","# Here Ratio of Test , Train and Split is 81:9:10\n","# It can be reset using changing the split variable in __init__ function\n","\n","class Standardize:\n","  def __init__(self, df, split=0.20):\n","    self.data=df\n","    self.split=split\n","  def split_data(self):\n","    n=int(len(self.data)*self.split)\n","    train, test=self.data.iloc[:-n], self.data.iloc[-n:]\n","    n=int(len(train)*self.split)\n","    train, val=train.iloc[:-n], train.iloc[-n:]\n","    assert len(test)+len(train)+len(val)==len(self.data)\n","    return train, test, val\n","    '''\n","  def split_data_view(self):\n","    n=int(len(self.data)*self.split)\n","    train, test=self.data.iloc[:-n], self.data.iloc[-n:]\n","    n=int(len(train)*self.split)\n","    train, val=train.iloc[:-n], train.iloc[-n:]\n","    assert len(test)+len(train)+len(val)==len(self.data)\n","    return train, val\n","    '''\n","  def transform(self, data):\n","    data_s=(data-self.mu)/self.sigma\n","    return data_s\n","  def fit_transform(self):\n","    train, test, val=self.split_data()\n","    self.mu, self.sigma= train.mean(),train.std()\n","    train_s=self.transform(train)\n","    test_s=self.transform(test)\n","    val_s=self.transform(val)\n","    return train_s, test_s, val_s\n","    '''\n","  def fit_transform_view(self):\n","    train, test, val=self.split_data_view()\n","    self.mu, self.sigma= train.mean(),train.std()\n","    train_s=self.transform(train)\n","    test_s=self.transform(test)\n","    val_s=self.transform(val)\n","    return train_s, test_s, val_s\n","    '''\n","  def inverse(self, data):\n","    return (data*self.sigma)+self.mu\n","  def inverse_y(self, data):\n","    return (data*self.sigma[-1])+self.mu[-1]\n","\n","\n","# This function deals with getting out the X and Y out of the dataset\n","# Column earlier named as 'y' will be taken out for processing, rest will be kept as x\n","\n","def features_target_ts(*args):\n","        y=[col.pop('y').values.reshape(-1,1) for col in args]\n","        #yk=[col.pop('var5(t)').values.reshape(-1,1) for col in args]\n","        x=[col.values.reshape(*col.shape,1) for col in args]\n","        return *y, *x\n","\n","\n","\n","#####################################################\n","################# Model Creation ####################\n","#####################################################\n","def create_model(train, units, dropout=0.2):\n","        # Variable-length int sequences.\n","\n","        \n","        model_LSTM = Sequential()  \n","        model_LSTM.add(LSTM(256,activation='relu',input_shape=(train.shape[1], train.shape[2])))\n","        model_LSTM.add(Dense(1))\n","        model_LSTM._name='LSTM'\n","                  \n","        model_GRU = Sequential()  \n","        model_GRU.add(GRU(256,activation='relu',input_shape=(train.shape[1], train.shape[2])))\n","        model_GRU.add(Dense(1))\n","        model_GRU._name='GRU'\n","                      \n","        model_RNN = Sequential()  \n","        model_RNN.add(SimpleRNN(256,activation='relu',input_shape=(train.shape[1], train.shape[2])))\n","        model_RNN.add(Dense(1))\n","        model_RNN._name='RNN'\n","                      \n","        model_BiLSTM = Sequential()  \n","        #model_BiLSTM.add(Bidirectional(LSTM(256,activation='relu',input_shape=(train.shape[1], train.shape[2]))))\n","        model_BiLSTM.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(train.shape[1], train.shape[2])))\n","        #print(model_BiLSTM.output_shape)\n","        model_BiLSTM.add(Flatten())\n","        #print(model_BiLSTM.output_shape)\n","        model_BiLSTM.add(Dense(1))\n","        #print(model_BiLSTM.output_shape)\n","        model_BiLSTM._name='BiLSTM'\n","        #model_BiLSTM.add(Dense(1))\n","                  \n","        model_CNN = Sequential()  \n","        model_CNN.add(Conv1D(filters=256, kernel_size=1,input_shape=(train.shape[1], train.shape[2])))\n","        model_CNN.add(Flatten())\n","        model_CNN.add(Dense(1))\n","        model_CNN._name='CNN'\n","        return  model_LSTM, model_GRU, model_CNN, model_RNN, model_BiLSTM\n","\n","# This function is responsible for model compilation and training \n","def train_model_ts(model, x_train, y_train, x_val, y_val, epochs=250, patience =5, batch_size=66):\n","          model.compile(optimizer='adam', loss='mean_squared_error', metrics=[RootMeanSquaredError(), MeanAbsoluteError()])\n","          es=keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=patience)\n","          history=model.fit(x_train, y_train, shuffle=False, epochs=epochs, batch_size=batch_size, validation_data=(x_val, y_val),callbacks=[es],verbose=0)\n","          return history\n","\n","# This function is responsible for Loss and Validation Loss Graph along with Test Prediction\n","def plot_forecast(model, x_test, y_test, index, history):\n","          #fig, ax=plt.subplots(2,1)\n","          #(pd.Series(history.history['loss']).plot(style='k', alpha=0.50, title='Loss By Epoch',ax=ax[0], label='loss'))\n","          #(pd.Series(history.history['val_loss']).plot(style='k', alpha=0.50, title='Loss By Epoch',ax=ax[0], label='val_loss'))\n","          #ax[0].legend()\n","          predicted=model.predict(x_test)\n","          #pd.Series(y_test.reshape(-1), index=index).plot(style='k--', alpha=0.5, ax=ax[1], title='Forecast Vs Actual', label='actual')\n","          #pd.Series(predicted.reshape(-1), index=index).plot(style='k', ax=ax[1],  label='Forecast')\n","          #fig.tight_layout()\n","          #ax[1].legend()\n","          #plt.show()\n","          return predicted\n","\n","def Measures(y_true,y_pred):\n","  '''All input in this function must be 'list' data structure'''\n","\n","  from sklearn.metrics import mean_squared_error\n","  from sklearn.metrics import mean_absolute_percentage_error\n","  from sklearn.metrics import mean_absolute_error\n","  import math\n","\n","  a=np.array(y_true)\n","  b=np.array(y_pred)\n","  rmse=math.sqrt(sum(pow(a-b,2))/len(a))\n","  mape=sum(abs((a-b)/b))/len(a)*100\n","\n","  #print('rmse:',rmse)\n","  #print('mape:',mape)\n","  return([rmse,mape])\n","\n","def DataTrainTestVal(df_new):   \n","    en_df=one_step_forecast(df_new, 12)\n","    #en_df\n","\n","    #Object of Standardize class has been created with dataset as the parameter\n","    scale_en=Standardize(en_df)\n","    train_en, test_en, val_en=scale_en.fit_transform()\n","    \n","    # Spliting into test, train and validation set\n","    y_train_en, y_val_en, y_test_in, x_train_en, x_val_en, x_test_en= features_target_ts(train_en, val_en, test_en)\n","    return (y_train_en, y_val_en, y_test_in, x_train_en, x_val_en, x_test_en, test_en, scale_en, en_df, val_en, train_en) \n","\n"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1686379941101,"user":{"displayName":"Dr. Vipin Kumar Computer Science \u0026 IT MGCUB","userId":"07997955097507838230"},"user_tz":-330},"id":"NNsgyK0_pRPc"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1686379941101,"user":{"displayName":"Dr. Vipin Kumar Computer Science \u0026 IT MGCUB","userId":"07997955097507838230"},"user_tz":-330},"id":"39JRpBY6B8QG"},"outputs":[],"source":["def MVL_Data_Generator(data_array,seasonality_min, view_max):\n","    data_length=len(data_array)\n","    Train_Data_Len=data_length*0.8\n","\n","    print(\"..........view number=\",view_max,\".................\")\n","    Views=[]\n","    ViewsDataset=[]\n","    seasonal=[seasonality_min]\n","\n","    multiply=round((Train_Data_Len/view_max)/seasonality_min)\n","\n","    seasonal=[seasonality_min]\n","    for i in seasonal:\n","      #print(\"Provided the Multiplication Factor: \"+str(multiply)+\" possible number of views is :\"+str(round((Train_Data_Len/multiply)/i))+\" for seasonality: \"+str(i))\n","      Views.append(round((Train_Data_Len/multiply)/i))\n","\n","    # creating view of the dataset.......................\n","    for j in seasonal:\n","      #print(len(data_array[-(j*multiply):-1].tolist()))\n","      ViewsDataset.append(data_array[-(j*multiply):-1].tolist())\n","      for i in range(1,view_max):\n","        #print(len(data_array[-(j*multiply)*(i+1):-(j*multiply)*(i)].tolist()))\n","        ViewsDataset.append(data_array[-(j*multiply)*(i+1):-(j*multiply)*(i)].tolist())\n","      #k+=1\n","    return(ViewsDataset)\n","\n","#Finding the Seasonal minimum values................\n","\n","def Seasonality_min_val(df):\n","    fh = 50 # or alternately fh = np.arange(1,13)\n","    fold = 5\n","\n","    fig_kwargs={'renderer': 'notebook'}\n","    eda = TSForecastingExperiment()\n","\n","    dd=eda.setup(data=df, fh=fh, fig_kwargs=fig_kwargs)\n","\n","    k=pd.DataFrame(dd.significant_sps)\n","\n","    seasonality=k.to_numpy().flatten()\n","    seasonality_min=np.amin(seasonality)\n","    return(seasonality_min)\n","\n","\n"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1686379941102,"user":{"displayName":"Dr. Vipin Kumar Computer Science \u0026 IT MGCUB","userId":"07997955097507838230"},"user_tz":-330},"id":"5LR3J3GkEQc9"},"outputs":[],"source":["def Model_Execution(models,x_train_en, y_train_en, x_val_en, y_val_en, x_test_en, y_test_in, test_en, scale_en, en_df, val_en, train_en, batch_size=64):\n","  view_accuracy=[]\n","  Test_Predictions=pd.DataFrame()\n","  Train_Predictions=pd.DataFrame()\n","  for i in models:\n","              #i.summary()\n","              print(i._name)\n","              history_en_proposed=train_model_ts(i, x_train_en, y_train_en, x_val_en, y_val_en,  batch_size=64)\n","              Test_len=len(y_test_in)\n","              #print(\"training History=\", history_en_proposed)\n","\n","\n","              #print(\"Model Trained !!! \")\n","\n","              test_predicted=plot_forecast(i, x_test_en, y_test_in, test_en.index, history_en_proposed)\n","\n","              scaled_test_y=scale_en.inverse_y(y_test_in)\n","              #print(\"Model Predicted For Test Data!!! \")\n","              #print('len(x_test_en)',len(scaled_test_y))\n","              #print('x_test_en values:\\n',scaled_test_y)\n","\n","              \n","              scaled_Pred=scale_en.inverse_y(test_predicted)\n","              Act_Pred=scaled_Pred\n","              scaled_Pred=pd.DataFrame(scaled_Pred)\n","              ll=en_df.iloc[-Test_len:,-1:]\n","\n","              RMSE_New=(mean_squared_error(scaled_Pred, ll)**0.5)\n","              #print('Data views number ===\u003e',serial,'\u003c========','RMSE_New:',RMSE_New)\n","              view_accuracy.append(RMSE_New)\n","              \n","              scaled_Pred=scaled_Pred.rename(columns={0:'hb'})\n","              ll['pd']=scaled_Pred['hb'].values\n","              ll.plot()\n","\n","              val_prediction=plot_forecast(i, x_val_en, y_val_en, val_en.index, history_en_proposed)\n","              #print(\"Validation Data Predicted !!! \")\n","              scaled_valid_Pred=scale_en.inverse_y(val_prediction)\n","              valid_performance=Measures(y_val_en,scaled_valid_Pred)\n","              #print(\"..................valid_performance...........===\",valid_performance)\n","\n","              Training_prediction=plot_forecast(i, x_train_en, y_train_en, train_en.index, history_en_proposed)\n","              #print(\"Training Data Predicted !!! \")\n","\n","              Total_Train_Prediction=np.append(Training_prediction, val_prediction)\n","              scaled_Train_Pred=scale_en.inverse_y(Total_Train_Prediction)\n","              Act_Train_Pred=scaled_Train_Pred\n","\n","              #print(\"Model To be Executed :\")\n","              #print(i)\n","              if i._name=='Proposed':\n","                  Test_Predictions['Proposed']=pd.DataFrame(Act_Pred)\n","                  Train_Predictions['Proposed']=pd.DataFrame(Act_Train_Pred)\n","              if i._name=='LSTM':\n","                  Test_Predictions['LSTM']=pd.DataFrame(Act_Pred)\n","                  Train_Predictions['LSTM']=pd.DataFrame(Act_Train_Pred)  \n","              if i._name=='GRU':\n","                  Test_Predictions['GRU']=pd.DataFrame(Act_Pred)\n","                  Train_Predictions['GRU']=pd.DataFrame(Act_Train_Pred)\n","              if i._name=='CNN':\n","                  Test_Predictions['CNN']=pd.DataFrame(Act_Pred)\n","                  Train_Predictions['CNN']=pd.DataFrame(Act_Train_Pred)\n","              if i._name=='RNN':\n","                  Test_Predictions['RNN']=pd.DataFrame(Act_Pred)\n","                  Train_Predictions['RNN']=pd.DataFrame(Act_Train_Pred)\n","              if i._name=='BiLSTM':\n","                  Test_Predictions['BiLSTM']=pd.DataFrame(Act_Pred)\n","                  Train_Predictions['BiLSTM']=pd.DataFrame(Act_Train_Pred)\n","  return(view_accuracy,valid_performance,Test_Predictions.values.tolist(),scaled_test_y, Total_Train_Prediction)\n","\n","\n","#Ensembling the model for multi-view learning....................................................................\n","\n","def MultiviewEnsemble(Traditional_model_pred,view_predictions, y_test_original,valid_performance_val, view_max):\n","    \n"," \n","    #traditional model prediction value adjustment\n","    model_pred=Traditional_model_pred[0][:][:]\n","    Trad_model_pred=[]\n","    for k in model_pred:\n","      Trad_model_pred.append(k[0])\n","\n","    #Multi-view ensemble of the predictions value adjustment\n","    \n","    views_model_pred=[]\n","    for l in range(len(view_predictions)):\n","      views_pred=view_predictions[l][:][:]\n","      #print(\"views_pred====\u003e\",views_pred,\"view(l)====\u003e\",l)\n","      views_model_pred_i=[]\n","      for k in views_pred:\n","        views_model_pred_i.append(k[0])\n","      views_model_pred.append(views_model_pred_i)\n","    \n","    p=np.transpose(np.array(views_model_pred))\n","    mean_val_MVL=np.mean(p, axis=1)\n","    MVL_pred=mean_val_MVL\n","    #------------------------------------\n","  \n","    #converting y_true original in the list formate\n","    y_true=[]\n","    for i in range(len(y_test_original)):\n","      y_true.append(y_test_original[i][0].tolist())\n","\n","    #checking the performances\n","    #print(\"len(y_true)\",len(y_true),\"len(Trad_model_pred)\",len(Trad_model_pred))\n","\n","    performance_model_trad=Measures(y_true,Trad_model_pred)\n","    #performance_MVL=Measures(y_true,MVL_pred)\n","\n","    print(\"Traditional model performance:\",performance_model_trad)\n","    #print(\"performance_MVL_without weighting:\",performance_MVL)\n","\n","    print(\"Validation performaces of views:\", valid_performance_val)\n","\n","    ###---------------------------------validation weighted performance-------------\n","     \n","    from sklearn.preprocessing import MinMaxScaler\n","\n","    weights=np.array(valid_performance_val[1:])\n","    validation_weighted_means=[]\n","    for array in p:\n","      #print('array===\u003e',array)\n","      #print('weighted sum:',(array*(weights/weights.sum())).sum())\n","      validation_weighted_means.append((array*(weights/weights.sum())).sum())\n","\n","    performance_MVL_valid_weighted=Measures(y_true,validation_weighted_means)\n","    #print(\"validation Weighted Performance:\", performance_MVL_valid_weighted)\n","    \n","    #2^x weighting method...........................................\n","\n","    wt=[]\n","    wt_sum=0\n","    new_p=p\n","    valid_performance_val_RMSE=valid_performance_val[1:]\n","    p_zero_mat = np.zeros((len(p), 1))\n","\n","    for q in range(view_max):\n","      wt=np.power( 2.71828,q)\n","      #print('wt=',wt)\n","      wt_sum=wt_sum+wt\n","      max_value = max(valid_performance_val_RMSE)\n","      max_index = valid_performance_val_RMSE.index(max_value)\n","      #print('index',max_index)\n","      #print('before wt', new_p[:,max_index],'indexed_=',max_index)\n","      #print('after wt', new_p[:,max_index]*wt,'indexed_=',max_index)\n","\n","      p_zero_mat= np.column_stack((p_zero_mat, new_p[:,max_index]*wt))\n","      #p_i_wt=new_p[:,max_index]*wt\n","      valid_performance_val_RMSE[max_index]=0\n","      #accessing the predicted values and get multiplied with their weighte and append in new new _p_wtd\n","    \n","       \n","    weighted_means=[]\n","    for array in p_zero_mat[:,1:]:\n","      weighted_means.append((array.sum()/wt_sum))\n","\n","    performance_MVL_valid_weighted_expo_2power=Measures(y_true,weighted_means)\n","    print(\"performance_MVL_valid_weighted_expo_2power:\", performance_MVL_valid_weighted_expo_2power)\n","\n","    #view wise accuracies.....\n","    print(\"view_Performance=\", view_accuracy)\n","\n","    return([view_max, performance_model_trad, performance_MVL_valid_weighted_expo_2power],[y_true,Trad_model_pred,weighted_means])\n","\n","#This function calculate the gradient biase of the predictions.....................................\n","def GradientBias(train, train_predict,y_predict):\n","  diff=[]\n","  grad=[]\n","  #print('train len=',len(train),'train_predict len=', len(train_predict))\n","  for i in range(len(train)-1):\n","    diff.append(train[i]-train_predict[i])\n","    #print('diff:',train[i],'-',train_predict[i],'=', train[i]-train_predict[i])\n","    grad.append(train_predict[i+1]-train_predict[i])\n","    #print('grad:',train_predict[i+1],'-',train_predict[i],'=', train_predict[i+1]-train_predict[i])\n","\n","  test_grad=[]\n","  #print(\"y_predict in GAB Fucntion:\", y_predict)\n","  #print(\"train_predict in GAB Fucntion:\", train_predict)\n","  #print(\"train in GAB Fucntion:\", train)\n","\n","  for i in range(len(y_predict)-1):\n","    test_grad.append(y_predict[i+1]-y_predict[i])\n","\n","  #regression for the prediction\n","  final_diff=RegFun(grad,diff,test_grad)\n","  final_pred=[]\n","  for i in range(len(final_diff)):\n","    final_pred.append(y_predict[i]+final_diff[i])\n","  return(final_pred)\n","\n","\n","def RegFun(X,y,test_grad):\n","  from sklearn.ensemble import AdaBoostRegressor\n","  regr = AdaBoostRegressor(random_state=0, n_estimators=100)\n","  y=np.array(y)\n","  X=np.array(X).reshape(len(X),1)\n","  test_x=np.array(test_grad).reshape(len(test_grad),1)\n","  regr.fit(X,y)\n","  final_diff=regr.predict(test_x)\n","  return(final_diff)\n","\n","# return of important data for saving...........!!!!!!!!!!\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1jnbGFkLuS46IAAztDt3AEkmdXB3tHVRD"},"id":"04FZqzGm1v_Z","outputId":"8ce41160-c694-4fc3-b0f6-3db51c80bfac"},"outputs":[],"source":["\n","seasonality_min=Seasonality_min_val(df)\n","data_array=df.to_numpy()\n","\n","\n","Results=[]\n","Results_GAB=[]\n","\n","#..................\n","previous_error_trad=9999\n","previous_lowest_GAB_error=9999\n","\n","current_trad_model_prediction=[]\n","current_GAB_model_prediction=[]\n","#.....................\n","\n","\n","for view_max in range(Start_view, views_upto+1):\n","    ViewsDataset=MVL_Data_Generator(data_array,seasonality_min, view_max)\n","###################\n","    view_accuracy=[]\n","    Traditional_model_pred=[]\n","    valid_performance_val=[]\n","    df.columns = ['y']\n","\n","    y_train_en, y_val_en, y_test_in, x_train_en, x_val_en, x_test_en, test_en, scale_en, en_df, val_en, train_en =DataTrainTestVal(df)\n","    models=[]\n","\n","    model_LSTM, model_GRU, model_CNN, model_RNN, model_BiLSTM =create_model(x_train_en, units=32)\n","\n","    if PredictorName == 'BiLSTM':\n","      models.append(model_BiLSTM)\n","    if PredictorName =='LSTM':\n","      models.append(model_LSTM)\n","    if PredictorName =='GRU':\n","      models.append(model_GRU)\n","    if PredictorName =='CNN':\n","      models.append(model_CNN)\n","    if PredictorName =='RNN':\n","      models.append(model_RNN)\n","\n","    view_accuracy,valid_performance_i,Test_Predictions,y_test_original,Total_Train_Prediction=Model_Execution(models,x_train_en, y_train_en, x_val_en, y_val_en, x_test_en, y_test_in, test_en, scale_en, en_df, val_en, train_en, batch_size=64)\n","\n","    Traditional_model_pred.append(Test_Predictions)\n","    valid_performance_val.append(valid_performance_i[0])\n","\n","    #########################################  Multi-view Time Series Analysis  #######################\n","    serial=0\n","    view_predictions=[]\n","    GAB_final_predictions=[]\n","\n","    for view in ViewsDataset:\n","      print('length of the views:',len(view))\n","      df_new = pd.DataFrame(view)\n","      df_new.columns = ['y']\n","      y_train_en, y_val_en, y_test_in_na, x_train_en, x_val_en, x_test_en_na, test_en_na, scale_en, en_df, val_en_na, train_en_na =DataTrainTestVal(df)\n","      #y_train_en, y_val_en, y_test_in_na, x_train_en, x_val_en, x_test_en_na=DataTrainTestVal(df_new)\n","\n","      y_train=np.concatenate((y_train_en, y_val_en), axis=0)\n","      x_train=np.concatenate((x_train_en, x_val_en), axis=0)\n","\n","      Train_combined = pd.concat([train_en, val_en], ignore_index=True)\n","      \n","      y_val=y_test_in_na\n","      x_val=x_test_en_na\n","\n","      Test_Predictions=pd.DataFrame()\n","      Train_Predictions=pd.DataFrame()\n","      \n","      print('Data views number ============\u003e',serial,'/',view_max,'\u003c================')\n","      serial=serial+1 \n","      \n","      #print(\"len(y_train_en)=\",y_train_en, \"len(x_train_en)=\", x_train_en)\n","\n","      #view_accuracy.append(Model_Execution(models,x_train, y_train, x_val, y_val,x_test_en, y_test_in, batch_size=64))\n","      # test_en_na has been utilized for validation purpose of specific view\n","      view_accuracy_i,valid_performance_i,Test_Predictions_i,ytest_original, Total_Train_Prediction=Model_Execution(models, x_train, y_train, x_val, y_val, x_test_en, y_test_in, test_en, scale_en, en_df, test_en_na, Train_combined, batch_size=64)\n","\n","      view_accuracy.append(view_accuracy_i)\n","      #print('shape of predictions:',np.array(Test_Predictions_i).shape)\n","      view_predictions.append(Test_Predictions_i)\n","      valid_performance_val.append(valid_performance_i[0])\n","\n","\n","      Total_Train=np.append(y_train, y_val)\n","\n","      #converting in single list of \"Test_Predictions_i\" variable\n","      Test_Predictions_ii=[]\n","      for tt in Test_Predictions_i:\n","        Test_Predictions_ii.append(tt[0])\n","\n","      GAB_final_predictions_i=GradientBias(Total_Train, Total_Train_Prediction, Test_Predictions_ii)\n","      GAB_final_predictions_i.append(GAB_final_predictions_i[-1])\n","\n","      GAB_final_predictions.append(GAB_final_predictions_i)\n","      #print(\"GAB_final_predictions_i=\",GAB_final_predictions_i)\n","    #view_predictions.append(ytest_original.tolist())\n","    #y_train_en, y_val_en, y_test_in, x_train_en, x_val_en, x_test_en=DataTrainTestVal(df)\n","\n","    #required variable===\u003e  Total_Train, Total_Train_Prediction,Test_Predictions_i,ytest_original\n","    #print(\"Training Length\"+str(Total_Train.shape()))\n","    #print(\"Training Prediction Length\"+str(Total_Train.shape()))\n","    #print(\"GAB_final_predictions==========\u003e\u003e\u003e\u003e\u003e\u003e\u003e\",GAB_final_predictions)\n","\n","\n","\n","    # Pure multi-view learning results ....................................................................................\n","    Results_i, prediction_test_i=MultiviewEnsemble(Traditional_model_pred,view_predictions, y_test_original,valid_performance_val, view_max)\n","\n","    #Formate preparation as per view_predictions format to GAB_final_predictions\n","    view_predictions_GAB=[]\n","    for zz in GAB_final_predictions:\n","      view_predictions_GAB.append([[x] for x in zz])\n","    # Pure multi-view+GAB learning results ....................................................................................\n","    GAB_results_i, prediction_test_i_GAB=MultiviewEnsemble(Traditional_model_pred,view_predictions_GAB, y_test_original,valid_performance_val, view_max)\n","\n","    Results.append(Results_i)\n","    Results_GAB.append(GAB_results_i)\n","\n","\n","    # Storing the \"Traditional_model_pred,view_predictions_GAB, y_test_original\" variables for the best performance\n","    view_max_i_GAB_current=GAB_results_i[0]\n","    RMSE_model_trad_i_current=GAB_results_i[1][0]\n","    RMSE_MVL_valid_weighted_expo_2power_i_GAB_current=GAB_results_i[2][0]\n","\n","\n","    # Traditional model predictions of best_lowest RMSE performance............\n","    if (RMSE_model_trad_i_current \u003c previous_error_trad):\n","          previous_error_trad=RMSE_model_trad_i_current          \n","          y_test_trad_predictions=prediction_test_i_GAB[1]\n","\n","    # GAB model predictions of best_lowest RMSE performance............\n","    if (RMSE_MVL_valid_weighted_expo_2power_i_GAB_current\u003cprevious_lowest_GAB_error):\n","          previous_lowest_GAB_error=RMSE_MVL_valid_weighted_expo_2power_i_GAB_current\n","          current_views=GAB_results_i[0]\n","          y_true=prediction_test_i_GAB[0]          \n","          y_test_GAB_model_predictions=prediction_test_i_GAB[2]\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1d6H7_YAYkkn"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"aIQCoXLOVbVk"},"outputs":[],"source":["import pickle\n","import os\n","\n","# Define the path components\n","path_drive = '/content/drive/MyDrive/AI LAB CoLab/Multi-view time series analysis/GAB_Results/MVL-GAB_Results_CNN'\n","filename = Dataset_list[Dataset_number]+'_'+ PredictorName +'.pkl'\n","file_path_save = os.path.join(path_drive, filename)\n","\n","Results_dictionary={'Dataset_name':Dataset_list[Dataset_number],'GAB_results':Results_GAB,'best_views':current_views, 'y_test':prediction_test_i_GAB[0],'Trad_predictions':prediction_test_i_GAB[1],'GAB_predictions':prediction_test_i_GAB[1]}\n","with open(file_path_save, 'wb') as file:\n","    pickle.dump(Results_dictionary, file)\n"]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}